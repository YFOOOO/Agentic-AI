#!/usr/bin/env python3
"""
RAGç³»ç»Ÿé›†æˆæµ‹è¯•
æµ‹è¯•å‘é‡å­˜å‚¨ã€æ–‡æ¡£å¤„ç†å’ŒçŸ¥è¯†æ£€ç´¢çš„åŸºç¡€åŠŸèƒ½
"""

import os
import sys
import tempfile
import shutil
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.rag import VectorStore, DocumentProcessor, KnowledgeRetriever

def test_document_processor():
    """æµ‹è¯•æ–‡æ¡£å¤„ç†å™¨åŠŸèƒ½"""
    print("ğŸ” æµ‹è¯•æ–‡æ¡£å¤„ç†å™¨...")
    
    processor = DocumentProcessor()
    
    # æµ‹è¯•æ–‡æœ¬
    test_text = """
    äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligence, AIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚
    
    æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ”¹è¿›ã€‚æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œ
    ä½¿ç”¨ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ã€‚
    
    è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ˜¯äººå·¥æ™ºèƒ½çš„å¦ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œä¸“æ³¨äºä½¿è®¡ç®—æœºèƒ½å¤Ÿç†è§£ã€è§£é‡Šå’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚
    """
    
    # æµ‹è¯•æ–‡æœ¬æ¸…ç†
    cleaned_text = processor.clean_text(test_text)
    print(f"âœ… æ–‡æœ¬æ¸…ç†å®Œæˆï¼Œé•¿åº¦: {len(cleaned_text)}")
    
    # æµ‹è¯•tokenè®¡æ•°
    token_count = processor.count_tokens(cleaned_text)
    print(f"âœ… Tokenè®¡æ•°: {token_count}")
    
    # æµ‹è¯•æ–‡æœ¬åˆ†å—
    chunks = processor.split_text(cleaned_text)
    print(f"âœ… æ–‡æœ¬åˆ†å—å®Œæˆï¼Œå…± {len(chunks)} ä¸ªå—")
    
    # æµ‹è¯•å…ƒæ•°æ®æå–
    metadata = processor.extract_metadata(cleaned_text, source="test_document")
    print(f"âœ… å…ƒæ•°æ®æå–å®Œæˆ: {metadata}")
    
    return True

def test_vector_store():
    """æµ‹è¯•å‘é‡å­˜å‚¨åŠŸèƒ½"""
    print("\nğŸ” æµ‹è¯•å‘é‡å­˜å‚¨...")
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•
    temp_dir = tempfile.mkdtemp()
    
    try:
        # åˆå§‹åŒ–å‘é‡å­˜å‚¨
        vector_store = VectorStore(
            collection_name="test_collection",
            persist_directory=temp_dir
        )
        
        # æµ‹è¯•æ–‡æ¡£
        documents = [
            "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„é‡è¦åˆ†æ”¯",
            "æœºå™¨å­¦ä¹ ä½¿è®¡ç®—æœºèƒ½å¤Ÿè‡ªåŠ¨å­¦ä¹ ",
            "æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œæ¨¡æ‹Ÿäººè„‘",
            "è‡ªç„¶è¯­è¨€å¤„ç†ä¸“æ³¨äºç†è§£äººç±»è¯­è¨€"
        ]
        
        metadatas = [
            {"source": "ai_intro", "topic": "AI"},
            {"source": "ml_intro", "topic": "ML"},
            {"source": "dl_intro", "topic": "DL"},
            {"source": "nlp_intro", "topic": "NLP"}
        ]
        
        # æ·»åŠ æ–‡æ¡£
        vector_store.add_documents(documents, metadatas)
        print(f"âœ… æˆåŠŸæ·»åŠ  {len(documents)} ä¸ªæ–‡æ¡£")
        
        # æµ‹è¯•ç›¸ä¼¼æ€§æœç´¢
        query = "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ "
        results = vector_store.search(query, n_results=2)
        print(f"âœ… ç›¸ä¼¼æ€§æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£")
        
        for i, result in enumerate(results):
            doc = result.get('document', '')
            score = result.get('similarity_score', 0)
            metadata = result.get('metadata', {})
            print(f"   ç»“æœ {i+1}: {doc[:50]}... (ç›¸ä¼¼åº¦: {score:.3f})")
        
        return True
        
    finally:
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        shutil.rmtree(temp_dir, ignore_errors=True)

def test_knowledge_retriever():
    """æµ‹è¯•çŸ¥è¯†æ£€ç´¢å™¨åŠŸèƒ½"""
    print("\nğŸ” æµ‹è¯•çŸ¥è¯†æ£€ç´¢å™¨...")
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•
    temp_dir = tempfile.mkdtemp()
    
    try:
        # åˆå§‹åŒ–çŸ¥è¯†æ£€ç´¢å™¨
        retriever = KnowledgeRetriever(
            collection_name="test_knowledge",
            persist_directory=temp_dir
        )
        
        # æ·»åŠ æµ‹è¯•æ–‡æ¡£
        test_documents = [
            "äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯æ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„è®¡ç®—æœºç³»ç»Ÿã€‚å®ƒåŒ…æ‹¬æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰å¤šä¸ªå­é¢†åŸŸã€‚",
            "æœºå™¨å­¦ä¹ æ˜¯AIçš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ï¼Œé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼ï¼Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚å¸¸è§ç®—æ³•åŒ…æ‹¬çº¿æ€§å›å½’ã€å†³ç­–æ ‘ã€ç¥ç»ç½‘ç»œç­‰ã€‚",
            "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å¤„ç†å¤æ‚æ•°æ®ã€‚å®ƒåœ¨å›¾åƒè¯†åˆ«ã€è¯­éŸ³è¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚",
            "è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸“æ³¨äºè®©è®¡ç®—æœºç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚ä¸»è¦ä»»åŠ¡åŒ…æ‹¬æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æã€æœºå™¨ç¿»è¯‘ã€é—®ç­”ç³»ç»Ÿç­‰ã€‚"
        ]
        
        # æ·»åŠ æ–‡æ¡£
        topics = ["AI", "ML", "DL", "NLP"]
        for i, doc in enumerate(test_documents):
            retriever.add_documents_from_text(
                doc, 
                sources=f"doc_{i}",
                metadata_list=[{"doc_id": f"doc_{i}", "topic": topics[i]}]
            )
        
        print(f"âœ… æˆåŠŸæ·»åŠ  {len(test_documents)} ä¸ªæ–‡æ¡£åˆ°çŸ¥è¯†åº“")
        
        # è·å–çŸ¥è¯†åº“ç»Ÿè®¡
        stats = retriever.get_knowledge_base_stats()
        print(f"âœ… çŸ¥è¯†åº“ç»Ÿè®¡: {stats}")
        
        # æµ‹è¯•æœç´¢
        query = "æ·±åº¦å­¦ä¹ åœ¨å“ªäº›é¢†åŸŸæœ‰åº”ç”¨"
        results = retriever.search(query, n_results=2)
        print(f"âœ… æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£")
        
        # æµ‹è¯•ä¸Šä¸‹æ–‡è·å–
        context_info = retriever.get_context_for_query(query, max_context_length=200)
        context = context_info.get('context', '')
        print(f"âœ… ä¸Šä¸‹æ–‡è·å–å®Œæˆï¼Œé•¿åº¦: {len(context)} å­—ç¬¦")
        print(f"   ä¸Šä¸‹æ–‡é¢„è§ˆ: {context[:100]}...")
        
        # æµ‹è¯•ç›¸å…³æŸ¥è¯¢å»ºè®®
        suggestions = retriever.suggest_related_queries(query, n_suggestions=3)
        print(f"âœ… æŸ¥è¯¢å»ºè®®: {suggestions}")
        
        return True
        
    finally:
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        shutil.rmtree(temp_dir, ignore_errors=True)

def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹RAGç³»ç»Ÿé›†æˆæµ‹è¯•\n")
    
    try:
        # è¿è¡Œå„é¡¹æµ‹è¯•
        success = True
        success &= test_document_processor()
        success &= test_vector_store()
        success &= test_knowledge_retriever()
        
        if success:
            print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼RAGç³»ç»ŸåŸºç¡€åŠŸèƒ½æ­£å¸¸")
            print("\nğŸ“Š æµ‹è¯•æ€»ç»“:")
            print("   - æ–‡æ¡£å¤„ç†å™¨: âœ… æ–‡æœ¬æ¸…ç†ã€åˆ†å—ã€å…ƒæ•°æ®æå–")
            print("   - å‘é‡å­˜å‚¨: âœ… æ–‡æ¡£åµŒå…¥ã€å­˜å‚¨ã€ç›¸ä¼¼æ€§æœç´¢")
            print("   - çŸ¥è¯†æ£€ç´¢å™¨: âœ… æ–‡æ¡£ç®¡ç†ã€æ™ºèƒ½æ£€ç´¢ã€ä¸Šä¸‹æ–‡ç”Ÿæˆ")
            return True
        else:
            print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
            return False
            
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)