"""
æµ‹è¯•å¤šæºæ•°æ®é›†æˆåŠŸèƒ½

éªŒè¯EnhancedLiteratureAgentä¸MultiSourceManagerçš„é›†æˆï¼Œ
ç¡®ä¿èƒ½å¤Ÿæ­£ç¡®åœ°ä»å¤šä¸ªæ•°æ®æºæ”¶é›†å’Œåˆå¹¶æ•°æ®ã€‚
"""

import os
import sys
import asyncio
import pandas as pd

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, '/Users/yf/Documents/Github_repository/Agentic-AI')

from src.agents.enhanced_literature_agent import EnhancedLiteratureAgent
from src.agents.multi_source_config_example import ACADEMIC_CONFIG


def test_multi_source_integration():
    """æµ‹è¯•å¤šæºæ•°æ®é›†æˆåŠŸèƒ½"""
    print("=== å¤šæºæ•°æ®é›†æˆåŠŸèƒ½æµ‹è¯• ===")
    
    # åˆ›å»ºæµ‹è¯•è¾“å‡ºç›®å½•
    test_output_dir = "test_output/multi_source"
    os.makedirs(test_output_dir, exist_ok=True)
    
    # åˆå§‹åŒ–å¢å¼ºç‰ˆæ–‡çŒ®ä»£ç†ï¼Œä½¿ç”¨å­¦æœ¯é…ç½®
    agent = EnhancedLiteratureAgent(
        base_dir=test_output_dir,
        multi_source_config=ACADEMIC_CONFIG
    )
    
    # å®šä¹‰æµ‹è¯•ä»»åŠ¡
    task = {
        "task_id": "multi_source_test_001",
        "query": "machine learning",
        "limit": 30
    }
    
    # æ‰§è¡Œä»»åŠ¡
    print(f"å¼€å§‹æ‰§è¡Œä»»åŠ¡: {task['task_id']}")
    print(f"æœç´¢æŸ¥è¯¢: {task['query']}")
    
    try:
        # è¿è¡Œå¼‚æ­¥ä»»åŠ¡
        result = asyncio.run(agent.handle(task))
        
        # éªŒè¯ç»“æœ
        print("\nä»»åŠ¡æ‰§è¡Œå®Œæˆï¼ŒéªŒè¯ç»“æœ...")
        
        # æ£€æŸ¥è¿”å›ç»“æœç»“æ„
        assert "type" in result, "ç»“æœç¼ºå°‘typeå­—æ®µ"
        assert result["type"] == "literature_collect", f"ç»“æœç±»å‹é”™è¯¯: {result['type']}"
        
        assert "task_id" in result, "ç»“æœç¼ºå°‘task_idå­—æ®µ"
        assert result["task_id"] == task["task_id"], f"ä»»åŠ¡IDä¸åŒ¹é…: {result['task_id']}"
        
        # æ£€æŸ¥artifacts
        assert "artifacts" in result, "ç»“æœç¼ºå°‘artifactså­—æ®µ"
        artifacts = result["artifacts"]
        
        assert "csv" in artifacts, "artifactsç¼ºå°‘csvå­—æ®µ"
        assert "report" in artifacts, "artifactsç¼ºå°‘reportå­—æ®µ"
        assert "run_dir" in artifacts, "artifactsç¼ºå°‘run_dirå­—æ®µ"
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        assert os.path.exists(artifacts["csv"]), f"CSVæ–‡ä»¶ä¸å­˜åœ¨: {artifacts['csv']}"
        assert os.path.exists(artifacts["report"]), f"æŠ¥å‘Šæ–‡ä»¶ä¸å­˜åœ¨: {artifacts['report']}"
        
        # æ£€æŸ¥metrics
        assert "metrics" in result, "ç»“æœç¼ºå°‘metricså­—æ®µ"
        metrics = result["metrics"]
        
        assert "rows" in metrics, "metricsç¼ºå°‘rowså­—æ®µ"
        assert "data_sources" in metrics, "metricsç¼ºå°‘data_sourceså­—æ®µ"
        
        # æ£€æŸ¥ä½¿ç”¨çš„æ•°æ®æº
        data_sources = result["data_sources"]
        assert "nobel_prize" in data_sources, "åº”åŒ…å«nobel_prizeæ•°æ®æº"
        assert len(data_sources) >= 2, f"åº”è‡³å°‘ä½¿ç”¨2ä¸ªæ•°æ®æºï¼Œå®é™…ä½¿ç”¨: {data_sources}"
        
        # æ£€æŸ¥CSVæ–‡ä»¶å†…å®¹
        df = pd.read_csv(artifacts["csv"])
        assert len(df) == metrics["rows"], f"CSVè¡Œæ•°ä¸metricsä¸åŒ¹é…: {len(df)} vs {metrics['rows']}"
        
        # æ£€æŸ¥æ•°æ®æºåˆ—
        assert "data_source" in df.columns, "CSVç¼ºå°‘data_sourceåˆ—"
        csv_sources = df["data_source"].unique().tolist()
        for source in data_sources:
            assert source in csv_sources, f"CSVä¸­ç¼ºå°‘æ•°æ®æº: {source}"
        
        print(f"\nâœ… æµ‹è¯•é€šè¿‡!")
        print(f"   - æ€»è®°å½•æ•°: {metrics['rows']}")
        print(f"   - ä½¿ç”¨æ•°æ®æº: {', '.join(data_sources)}")
        print(f"   - CSVæ–‡ä»¶: {artifacts['csv']}")
        print(f"   - æŠ¥å‘Šæ–‡ä»¶: {artifacts['report']}")
        
        # æ˜¾ç¤ºéƒ¨åˆ†æ•°æ®
        print("\næ•°æ®æºåˆ†å¸ƒ:")
        if "source_distribution" in metrics:
            for source, count in metrics["source_distribution"].items():
                print(f"   - {source}: {count} æ¡è®°å½•")
        
        print("\nå‰5æ¡è®°å½•é¢„è§ˆ:")
        print(df.head()[['title', 'data_source']].to_string(index=False))
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_backward_compatibility():
    """æµ‹è¯•å‘åå…¼å®¹æ€§"""
    print("\n=== å‘åå…¼å®¹æ€§æµ‹è¯• ===")
    
    # åˆ›å»ºæµ‹è¯•è¾“å‡ºç›®å½•
    test_output_dir = "test_output/compatibility"
    os.makedirs(test_output_dir, exist_ok=True)
    
    # ä½¿ç”¨åˆ«ååˆå§‹åŒ–ï¼ˆä¸å¯ç”¨å¤šæºæ•°æ®ï¼‰
    from src.agents.enhanced_literature_agent import LiteratureAgent
    agent = LiteratureAgent(base_dir=test_output_dir)
    
    # å®šä¹‰ç®€å•ä»»åŠ¡ï¼ˆæ— æŸ¥è¯¢ï¼‰
    task = {
        "task_id": "compatibility_test_001"
    }
    
    try:
        # è¿è¡Œå¼‚æ­¥ä»»åŠ¡
        result = asyncio.run(agent.handle(task))
        
        # éªŒè¯ç»“æœ
        assert "type" in result, "ç»“æœç¼ºå°‘typeå­—æ®µ"
        assert result["type"] == "literature_collect", f"ç»“æœç±»å‹é”™è¯¯: {result['type']}"
        
        # æ£€æŸ¥æ•°æ®æº
        data_sources = result["data_sources"]
        assert data_sources == ["nobel_prize"], f"å‘åå…¼å®¹æ€§æµ‹è¯•å¤±è´¥ï¼Œåº”åªåŒ…å«nobel_prizeæ•°æ®æº: {data_sources}"
        
        print(f"âœ… å‘åå…¼å®¹æ€§æµ‹è¯•é€šè¿‡!")
        print(f"   - ä½¿ç”¨æ•°æ®æº: {', '.join(data_sources)}")
        print(f"   - æ€»è®°å½•æ•°: {result['metrics']['rows']}")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ å‘åå…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹å¤šæºæ•°æ®é›†æˆåŠŸèƒ½æµ‹è¯•...")
    
    # æµ‹è¯•å¤šæºæ•°æ®é›†æˆ
    success1 = test_multi_source_integration()
    
    # æµ‹è¯•å‘åå…¼å®¹æ€§
    success2 = test_backward_compatibility()
    
    if success1 and success2:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
        return 0
    else:
        print("\nğŸ’¥ éƒ¨åˆ†æµ‹è¯•å¤±è´¥!")
        return 1


if __name__ == "__main__":
    exit(main())